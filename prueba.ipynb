{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the agent (VisualAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (1.65.4.post1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from litellm) (3.11.14)\n",
      "Requirement already satisfied: click in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from litellm) (8.1.8)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from litellm) (0.27.2)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from litellm) (8.6.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from litellm) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from litellm) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.68.2 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from litellm) (1.70.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from litellm) (2.10.6)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from litellm) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from litellm) (0.9.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from litellm) (0.21.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from httpx>=0.23.0->litellm) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from httpx>=0.23.0->litellm) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from httpx>=0.23.0->litellm) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from httpx>=0.23.0->litellm) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from httpx>=0.23.0->litellm) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.23.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from openai>=1.68.2->litellm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from openai>=1.68.2->litellm) (0.9.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from openai>=1.68.2->litellm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from openai>=1.68.2->litellm) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.27.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2.32.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from aiohttp->litellm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from aiohttp->litellm) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from aiohttp->litellm) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from aiohttp->litellm) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from aiohttp->litellm) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from aiohttp->litellm) (1.18.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from click->litellm) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from tokenizers->litellm) (0.30.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\david\\documents\\langgraph\\langchaincourse\\lc-academy-env\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip  install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Base ===\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Phoenix core ===\n",
    "import phoenix as px\n",
    "from phoenix.trace import SpanEvaluations\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "\n",
    "# === Evaluaciones autom√°ticas ===\n",
    "from phoenix.evals import (\n",
    "    TOOL_CALLING_PROMPT_TEMPLATE,\n",
    "    llm_classify,\n",
    "    PromptTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "# === LLM local (ej. llama.cpp o llamafile) ===\n",
    "#from phoenix.evals.models import \n",
    "\n",
    "# === Extra ===\n",
    "from openinference.instrumentation import suppress_tracing\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OLLAMA_API_BASE']= 'http://localhost:11434'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"evaluating-agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'langgraph.version' from 'c:\\\\Users\\\\david\\\\Documents\\\\LangGraph\\\\LangChainCourse\\\\lc-academy-env\\\\Lib\\\\site-packages\\\\langgraph\\\\version.py'>\n",
      "OpenTelemetry Tracing Details\n",
      "|  Phoenix Project: evaluating-agent\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'api_key': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import run_graph_with_tracing, start_main_span\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from phoenix.evals.models import BaseModel\n",
    "from typing import Optional, Sequence, Union, Any\n",
    "from langchain_ollama import ChatOllama\n",
    "import asyncio'\n",
    "'''\n",
    "\n",
    "from phoenix.evals import llm_classify, TOOL_CALLING_PROMPT_TEMPLATE, PromptTemplate, LiteLLMModel\n",
    "from litellm import completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Correct Phoenix Trace Querying ===\n",
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "# Define evaluation queries properly\n",
    "sql_query = (\n",
    "    SpanQuery()\n",
    "    .where(\"name == 'sql_query_exec' and span_kind == 'TOOL'\")\n",
    ").select(\n",
    "    question=\"input.value\",\n",
    "    query_gen=\"output.value\",\n",
    ")\n",
    "\n",
    "analysis_query = (\n",
    "    SpanQuery()\n",
    "    .where(\"name == 'data_analysis' and span_kind == 'TOOL'\")\n",
    ").select(\n",
    "    query=\"input.value\",\n",
    "    response=\"output.value\",\n",
    ")\n",
    "\n",
    "viz_query = (\n",
    "    SpanQuery()\n",
    "    .where(\"name == 'gen_visualization' and span_kind == 'TOOL'\")\n",
    ").select(\n",
    "    input=\"input.value\",\n",
    "    generated_code=\"output.value\",\n",
    ")\n",
    "\n",
    "decide_query = (\n",
    "    SpanQuery()\n",
    "    .where(\"span_kind == 'TOOL' and name == 'decide_tool'\")\n",
    ").select(\n",
    "    question=\"input.value\",\n",
    "    tool_call=\"output.value\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create loop to see multiple times the difference in the \"re-runs\"\n",
    "1. Use multiple type of prompts styles for testing\n",
    "2. Human in the loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. complete\n",
    "2. How to save a csv file with the output of the traces.\n",
    "3. Try to make a decision based on different branches and parallelization\n",
    "4. cook up a docker container\n",
    "5. Try access with the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Starting LangGraph execution with tracing\n",
      "Elecci√≥n de herramienta: lookup_sales_data\n",
      "What was the most popular product SKU?\n",
      "Elecci√≥n de herramienta: analyzing_data\n",
      "Elecci√≥n de herramienta: create_visualization\n",
      "Elecci√≥n de herramienta: analyzing_data\n",
      "Elecci√≥n de herramienta: end\n",
      "‚úÖ LangGraph execution completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  20%|‚ñà‚ñà        | 1/5 [00:30<02:01, 30.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Starting LangGraph execution with tracing\n",
      "Elecci√≥n de herramienta: lookup_sales_data\n",
      "What was the total revenue across all stores?\n",
      "Elecci√≥n de herramienta: analyzing_data\n",
      "Elecci√≥n de herramienta: analyzing_data\n",
      "Elecci√≥n de herramienta: end\n",
      "‚úÖ LangGraph execution completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:46<01:06, 22.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Starting LangGraph execution with tracing\n",
      "Elecci√≥n de herramienta: lookup_sales_data\n",
      "Which store had the highest sales volume?\n",
      "Elecci√≥n de herramienta: analyzing_data\n",
      "Elecci√≥n de herramienta: lookup_sales_data\n",
      "Which store had the highest sales volume?\n",
      "Elecci√≥n de herramienta: analyzing_data\n",
      "Elecci√≥n de herramienta: end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:08<00:44, 22.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangGraph execution completed\n",
      "üîÅ Starting LangGraph execution with tracing\n",
      "Elecci√≥n de herramienta: lookup_sales_data\n",
      "Create a bar chart showing total sales by store\n",
      "Elecci√≥n de herramienta: create_visualization\n",
      "Elecci√≥n de herramienta: analyzing_data\n",
      "Elecci√≥n de herramienta: create_visualization\n",
      "Elecci√≥n de herramienta: analyzing_data\n",
      "Elecci√≥n de herramienta: end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [01:50<00:29, 29.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangGraph execution completed\n",
      "üîÅ Starting LangGraph execution with tracing\n",
      "Elecci√≥n de herramienta: lookup_sales_data\n",
      "What was the average transaction value?\n",
      "Elecci√≥n de herramienta: analyzing_data\n",
      "Elecci√≥n de herramienta: analyzing_data\n",
      "Elecci√≥n de herramienta: end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:00<00:00, 24.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangGraph execution completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "agent_questions = [\n",
    "    \"What was the most popular product SKU?\",\n",
    "    \"What was the total revenue across all stores?\",\n",
    "    \"Which store had the highest sales volume?\",\n",
    "    \"Create a bar chart showing total sales by store\",\n",
    "    \"What was the average transaction value?\"\n",
    "]\n",
    "\n",
    "for question in tqdm(agent_questions, desc=\"Processing questions\"):\n",
    "    try:\n",
    "        input_state = {\n",
    "            \"prompt\": question,\n",
    "        }\n",
    "        ret = run_graph_with_tracing(input_state)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\")\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts for evaluation of the tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SQL Generation Evaluation ===\n",
    "SQL_EVAL_GEN_PROMPT = \"\"\"\n",
    "SQL Evaluation Prompt:\n",
    "-----------------------\n",
    "You are tasked with determining if the SQL generated appropiately answers a given instruction\n",
    "taking into account its generated query and response.\n",
    "\n",
    "Data:\n",
    "-----\n",
    "- [Instruction]: {question}\n",
    "  This section contains the specific task or problem that the sql query is intended to solve.\n",
    "\n",
    "- [Reference Query]: {query_gen}\n",
    "  This is the sql query submitted for evaluation. Analyze it in the context of the provided\n",
    "  instruction.\n",
    "\n",
    "Evaluation:\n",
    "-----------\n",
    "Your response should be a single word: either \"correct\" or \"incorrect\".\n",
    "You must assume that the db exists and that columns are appropiately named.\n",
    "You must take into account the response as additional information to determine the correctness.\n",
    "\n",
    "- \"correct\" indicates that the sql query correctly solves the instruction.\n",
    "- \"incorrect\" indicates that the sql query correctly does not solve the instruction correctly.\n",
    "\n",
    "Note: Your response should contain only the word \"correct\" or \"incorrect\" with no additional text\n",
    "or characters.\n",
    "\"\"\"\n",
    "\n",
    "# === Data Analysis Evaluation ===\n",
    "CLARITY_LLM_JUDGE_PROMPT = \"\"\"\n",
    "In this task, you will be presented with a query and an answer. Your objective is to evaluate the clarity \n",
    "of the answer in addressing the query. A clear response is one that is precise, coherent, and directly \n",
    "addresses the query without introducing unnecessary complexity or ambiguity. An unclear response is one \n",
    "that is vague, disorganized, or difficult to understand, even if it may be factually correct.\n",
    "\n",
    "Your response should be a single word: either \"clear\" or \"unclear,\" and it should not include any other \n",
    "text or characters. \"clear\" indicates that the answer is well-structured, easy to understand, and \n",
    "appropriately addresses the query. \"unclear\" indicates that some part of the response could be better \n",
    "structured or worded.\n",
    "Please carefully consider the query and answer before determining your response.\n",
    "\n",
    "After analyzing the query and the answer, you must write a detailed explanation of your reasoning to \n",
    "justify why you chose either \"clear\" or \"unclear.\" Avoid stating the final label at the beginning of your \n",
    "explanation. Your reasoning should include specific points about how the answer does or does not meet the \n",
    "criteria for clarity.\n",
    "\n",
    "[BEGIN DATA]\n",
    "Query: {query}\n",
    "Answer: {response}\n",
    "[END DATA]\n",
    "Please analyze the data carefully and provide an explanation followed by your response.\n",
    "\n",
    "EXPLANATION: Provide your reasoning step by step, evaluating the clarity of the answer based on the query.\n",
    "LABEL: \"clear\" or \"unclear\"\n",
    "\"\"\"\n",
    "\n",
    "# === Visualization Evaluation ===\n",
    "VIZ_QUALITY_TEMPLATE = PromptTemplate(\"\"\"\n",
    "Evaluate this visualization configuration:\n",
    "1. Appropriateness of chart type for the data\n",
    "2. Correct mapping of axes\n",
    "3. Clarity of visualization goal\n",
    "\n",
    "Goal: {input}\n",
    "Data Sample: {reference_data}\n",
    "Configuration: {output}\n",
    "\n",
    "Respond with \"good\" or \"poor\" and a brief reason.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"name\": \"lookup_sales_data\",\n",
    "        \"description\": \"Fetch historical data of sales for a product or category.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"analyzing_data\",\n",
    "        \"description\": \"Does a statistical analysis of the data available, giving an output in form of a summary of trends/patterns found for example.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"create_visualization\",\n",
    "        \"description\": \"Generates a visualization schema of the data processed according to the user's configuration.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter\n",
    "from opentelemetry import trace\n",
    "\n",
    "# A√±ade exportador a consola para debug\n",
    "trace.get_tracer_provider().add_span_processor(\n",
    "    SimpleSpanProcessor(ConsoleSpanExporter())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an evaluation assistant evaluating questions and tool calls to\n",
      "determine whether the tool called would answer the question. The tool\n",
      "calls have been generated by a separate agent, and chosen from the list of\n",
      "tools provided below. It is your job to decide whether that agent chose\n",
      "the right tool to call.\n",
      "\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: {question}\n",
      "    ************\n",
      "    [Tool Called]: {tool_call}\n",
      "    [END DATA]\n",
      "\n",
      "Your response must be single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"incorrect\" means that the chosen tool would not answer the question,\n",
      "the tool includes information that is not presented in the question,\n",
      "or that the tool signature includes parameter values that don't match\n",
      "the formats specified in the tool signatures below.\n",
      "\n",
      "\"correct\" means the correct tool call was chosen, the correct parameters\n",
      "were extracted from the question, the tool call generated is runnable and correct,\n",
      "and that no outside information not present in the question was used\n",
      "in the generated question.\n",
      "\n",
      "    [Tool Definitions]: {tool_definitions}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(TOOL_CALLING_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60254224bdda4f8899f4121e0868b0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/73 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>explanation</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>execution_status</th>\n",
       "      <th>execution_seconds</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>faa0113f472fb26a</th>\n",
       "      <td>NOT_PARSABLE</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.919449</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522a99763361f627</th>\n",
       "      <td>NOT_PARSABLE</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>0.445795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5193ea1de96a7e4f</th>\n",
       "      <td>NOT_PARSABLE</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.435714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53a22c3a269b96d4</th>\n",
       "      <td>NOT_PARSABLE</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>0.412020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229a2f9084e4d308</th>\n",
       "      <td>NOT_PARSABLE</td>\n",
       "      <td>correct</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>0.247947</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         label explanation exceptions execution_status  \\\n",
       "context.span_id                                                          \n",
       "faa0113f472fb26a  NOT_PARSABLE   incorrect         []        COMPLETED   \n",
       "522a99763361f627  NOT_PARSABLE   incorrect         []        COMPLETED   \n",
       "5193ea1de96a7e4f  NOT_PARSABLE   incorrect         []        COMPLETED   \n",
       "53a22c3a269b96d4  NOT_PARSABLE   incorrect         []        COMPLETED   \n",
       "229a2f9084e4d308  NOT_PARSABLE     correct         []        COMPLETED   \n",
       "\n",
       "                  execution_seconds  score  \n",
       "context.span_id                             \n",
       "faa0113f472fb26a           2.919449      0  \n",
       "522a99763361f627           0.445795      0  \n",
       "5193ea1de96a7e4f           2.435714      0  \n",
       "53a22c3a269b96d4           0.412020      0  \n",
       "229a2f9084e4d308           0.247947      0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "#model = completion(model='ollama_chat/llama2', api_base=\"http://localhost:11434\", stream=False)\n",
    "#model.reload_client = lambda: None\n",
    "##model.verbose_generation_info = lambda: \"\"\n",
    "#model._timeout = lambda: 0\n",
    "model = LiteLLMModel(model=\"ollama_chat/llama3.2:3B\")\n",
    "input_state = {\"prompt\": \"Show me sales in Nov 2021\"}\n",
    "#result = run_graph_with_tracing(input_state)\n",
    "#pprint.pprint(result)\n",
    "\n",
    "#verify traces\n",
    "\n",
    "tool_calls_df = px.Client().query_spans(decide_query, project_name=PROJECT_NAME, timeout=None)\n",
    "tool_calls_df = tool_calls_df.dropna(subset=[\"tool_call\"])\n",
    "\n",
    "tool_calls_df.head()\n",
    "\n",
    "tool_call_eval = llm_classify(\n",
    "    dataframe=tool_calls_df,\n",
    "    template=TOOL_CALLING_PROMPT_TEMPLATE.template[0].template.replace(\n",
    "        \"{tool_definitions}\", json.dumps(tools).replace(\"{\", '\"').replace(\"}\", '\"')),\n",
    "    rails=['correct', 'incorrect'],\n",
    "    model=model,\n",
    "    provide_explanation=True,\n",
    "    concurrency=1,\n",
    ")\n",
    "\n",
    "tool_call_eval['score'] = tool_call_eval.apply(lambda x: 1 if x['label']=='correct' else 0, axis=1)\n",
    "\n",
    "tool_call_eval.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Tool Calling Eval\", dataframe=tool_call_eval),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>query_gen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f336001a171b6854</th>\n",
       "      <td>What was the most popular product SKU?</td>\n",
       "      <td>SKU_Coded\\n0    6200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca53fe2ff8052d77</th>\n",
       "      <td>What was the total revenue across all stores?</td>\n",
       "      <td>sum(Total_Sale_Value)\\n0           1.327264...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87feb9c9d85bad98</th>\n",
       "      <td>Which store had the highest sales volume?</td>\n",
       "      <td>Store_Number\\n0          2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f64e036ee56ba02</th>\n",
       "      <td>Which store had the highest sales volume?</td>\n",
       "      <td>Store_Number\\n0          2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b8cb9cb04f90ddc6</th>\n",
       "      <td>Create a bar chart showing total sales by store</td>\n",
       "      <td>Store_Number  sum(Total_Sale_Value)\\n0    ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         question  \\\n",
       "context.span_id                                                     \n",
       "f336001a171b6854           What was the most popular product SKU?   \n",
       "ca53fe2ff8052d77    What was the total revenue across all stores?   \n",
       "87feb9c9d85bad98        Which store had the highest sales volume?   \n",
       "5f64e036ee56ba02        Which store had the highest sales volume?   \n",
       "b8cb9cb04f90ddc6  Create a bar chart showing total sales by store   \n",
       "\n",
       "                                                          query_gen  \n",
       "context.span_id                                                      \n",
       "f336001a171b6854                            SKU_Coded\\n0    6200700  \n",
       "ca53fe2ff8052d77     sum(Total_Sale_Value)\\n0           1.327264...  \n",
       "87feb9c9d85bad98                      Store_Number\\n0          2970  \n",
       "5f64e036ee56ba02                      Store_Number\\n0          2970  \n",
       "b8cb9cb04f90ddc6      Store_Number  sum(Total_Sale_Value)\\n0    ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_df = px.Client().query_spans(sql_query, project_name=PROJECT_NAME, timeout=None)\n",
    "sql_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>query_gen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f336001a171b6854</th>\n",
       "      <td>What was the most popular product SKU?</td>\n",
       "      <td>SKU_Coded\\n0    6200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca53fe2ff8052d77</th>\n",
       "      <td>What was the total revenue across all stores?</td>\n",
       "      <td>sum(Total_Sale_Value)\\n0           1.327264...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87feb9c9d85bad98</th>\n",
       "      <td>Which store had the highest sales volume?</td>\n",
       "      <td>Store_Number\\n0          2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f64e036ee56ba02</th>\n",
       "      <td>Which store had the highest sales volume?</td>\n",
       "      <td>Store_Number\\n0          2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b8cb9cb04f90ddc6</th>\n",
       "      <td>Create a bar chart showing total sales by store</td>\n",
       "      <td>Store_Number  sum(Total_Sale_Value)\\n0    ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         question  \\\n",
       "context.span_id                                                     \n",
       "f336001a171b6854           What was the most popular product SKU?   \n",
       "ca53fe2ff8052d77    What was the total revenue across all stores?   \n",
       "87feb9c9d85bad98        Which store had the highest sales volume?   \n",
       "5f64e036ee56ba02        Which store had the highest sales volume?   \n",
       "b8cb9cb04f90ddc6  Create a bar chart showing total sales by store   \n",
       "\n",
       "                                                          query_gen  \n",
       "context.span_id                                                      \n",
       "f336001a171b6854                            SKU_Coded\\n0    6200700  \n",
       "ca53fe2ff8052d77     sum(Total_Sale_Value)\\n0           1.327264...  \n",
       "87feb9c9d85bad98                      Store_Number\\n0          2970  \n",
       "5f64e036ee56ba02                      Store_Number\\n0          2970  \n",
       "b8cb9cb04f90ddc6      Store_Number  sum(Total_Sale_Value)\\n0    ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = (SpanQuery()\n",
    "    .where(\"name == 'sql_query_exec' and span_kind == 'TOOL'\")\n",
    ").select(\n",
    "    question=\"input.value\",\n",
    "    query_gen=\"output.value\",\n",
    ")\n",
    "query_df = px.Client().query_spans(query, project_name=PROJECT_NAME, timeout=None)\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluator for the other tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae0218d29fd4f14a8ae79167442c3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/6 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === SQL Generation Evaluation ===\n",
    "\n",
    "sql_df = px.Client().query_spans(sql_query, project_name=PROJECT_NAME, timeout=None)\n",
    "#sql_df = sql_df[sql_df[\"question\"].str.contains(\"Generate an SQL query based on a prompt.\", na=False)]\n",
    "\n",
    "with suppress_tracing():\n",
    "    sql_eval = llm_classify(\n",
    "        dataframe=sql_df,\n",
    "        template=SQL_EVAL_GEN_PROMPT,\n",
    "        rails=[\"correct\", \"incorrect\"],\n",
    "        model=model,\n",
    "        provide_explanation=True\n",
    "    )\n",
    "\n",
    "sql_eval ['score'] = sql_eval.apply(lambda x: 1 if x['label']=='correct' else 0, axis=1)\n",
    "sql_eval.head()\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"SQL Generation Eval\", dataframe=sql_eval),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>query_gen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [question, query_gen]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fe05ae92764503a6e96d8936cd62d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/10 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Data Analysis Evaluation ===\n",
    "clarity_df = px.Client().query_spans(analysis_query, project_name=PROJECT_NAME, timeout=None)\n",
    "clarity_df.head()\n",
    "with suppress_tracing():\n",
    "    clarity_eval = llm_classify(\n",
    "        dataframe=clarity_df,\n",
    "        template=CLARITY_LLM_JUDGE_PROMPT,\n",
    "        rails=[\"clear\", \"unclear\"],\n",
    "        model=model,\n",
    "        provide_explanation=True\n",
    "    )\n",
    "clarity_eval['score'] = clarity_eval.apply(lambda x: 1 if x['label']=='clear' else 0, axis=1)\n",
    "\n",
    "clarity_eval.head()\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Response Clarity\", dataframe=clarity_eval),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization Evaluation ===\n",
    "code_gen_df = px.Client().query_spans(viz_query, project_name=PROJECT_NAME, timeout=None)\n",
    "code_gen_df.head()\n",
    "\n",
    "def code_is_runnable(output:str) -> bool:\n",
    "    output = output.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    try:\n",
    "        exec(output)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    \n",
    "code_gen_df['label'] = code_gen_df['generated_code'].apply(code_is_runnable).map({True: \"runnable\", False: \"not_runnable\"})\n",
    "code_gen_df['score'] = code_gen_df['label'].apply(lambda x: 1 if x=='runnable' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>generated_code</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [input, generated_code, label, score]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_gen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Runnable Code Eval\", dataframe=code_gen_df),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
